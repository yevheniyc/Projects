{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, tree, model_selection, metrics\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet - Answer - DGA Detection using Machine Learning\n",
    "\n",
    "This worksheet is a step-by-step guide on how to detect domains that were generated using \"Domain Generation Algorithm\" (DGA). We will walk you through the process of transforming raw domain strings to Machine Learning features and creating a decision tree classifer which you will use to determine whether a given domain is legit or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw domain strings to numeric Machine Learning features using DataFrame manipulations\n",
    "2. **Machine Learning Classification** - predict whether a domain is legit or not using a Decision Tree Classifier\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "**DGA - Background**\n",
    "\n",
    "\"Various families of malware use domain generation\n",
    "algorithms (DGAs) to generate a large number of pseudo-random\n",
    "domain names to connect to a command and control (C2) server.\n",
    "In order to block DGA C2 traffic, security organizations must\n",
    "first discover the algorithm by reverse engineering malware\n",
    "samples, then generate a list of domains for a given seed. The\n",
    "domains are then either preregistered, sink-holed or published\n",
    "in a DNS blacklist. This process is not only tedious, but can\n",
    "be readily circumvented by malware authors. An alternative\n",
    "approach to stop malware from using DGAs is to intercept DNS\n",
    "queries on a network and predict whether domains are DGA\n",
    "generated. Much of the previous work in DGA detection is based\n",
    "on finding groupings of like domains and using their statistical\n",
    "properties to determine if they are DGA generated. However,\n",
    "these techniques are run over large time windows and cannot be\n",
    "used for real-time detection and prevention. In addition, many of\n",
    "these techniques also use contextual information such as passive\n",
    "DNS and aggregations of all NXDomains throughout a network.\n",
    "Such requirements are not only costly to integrate, they may not\n",
    "be possible due to real-world constraints of many systems (such\n",
    "as endpoint detection). An alternative to these systems is a much\n",
    "harder problem: detect DGA generation on a per domain basis\n",
    "with no information except for the domain name. Previous work\n",
    "to solve this harder problem exhibits poor performance and many\n",
    "of these systems rely heavily on manual creation of features;\n",
    "a time consuming process that can easily be circumvented by\n",
    "malware authors...\"    \n",
    "[Citation: Woodbridge et. al 2016: \"Predicting Domain Generation Algorithms with Long Short-Term Memory Networks\"]\n",
    "\n",
    "A better alternative for real-world deployment would be to use \"featureless deep learning\" - We have a separate notebook where you can see how this can be implemented!\n",
    "\n",
    "**However, let's learn the basics first!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet for Part 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakpoint: Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply load the feature matrix we prepared for you, so you can move on to Part 2 and train a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1000\n",
      "0    1000\n",
      "Name: isDGA, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isDGA</th>\n",
       "      <th>length</th>\n",
       "      <th>digits</th>\n",
       "      <th>entropy</th>\n",
       "      <th>vowel-cons</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.546594</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>968.076729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>3.833270</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>481.067222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.855389</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1036.365657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>3.844107</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>708.328718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.084963</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>897.543434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isDGA  length  digits   entropy  vowel-cons       ngrams\n",
       "0      1      13       0  3.546594    0.300000   968.076729\n",
       "1      1      25      10  3.833270    0.250000   481.067222\n",
       "2      1      12       0  2.855389    0.090909  1036.365657\n",
       "3      1      26       6  3.844107    0.052632   708.328718\n",
       "4      1      12       0  3.084963    0.090909   897.543434"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.read_csv('../../data/dga_features_final_df.csv')\n",
    "print(df_final.isDGA.value_counts())\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of common english words from part 1\n",
    "from six.moves import cPickle as pickle\n",
    "with open('../../data/d_common_en_words' + '.pickle', 'rb') as f:\n",
    "        d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Machine Learning\n",
    "\n",
    "To learn simple classification procedures using [sklearn](http://scikit-learn.org/stable/) we have split the work flow into 5 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Feature matrix and ```target``` vector containing the URL labels\n",
    "\n",
    "- In statistics, the feature matrix is often referred to as ```X```\n",
    "- target is a vector containing the labels for each URL (often also called *y* in statistics)\n",
    "- In sklearn both the input and target can either be a pandas DataFrame/Series or numpy array/vector respectively (can't be lists!)\n",
    "\n",
    "Tasks:\n",
    "- assign 'isDGA' column to a pandas Series named 'target'\n",
    "- drop 'isDGA' column from ```dga``` DataFrame and name the resulting pandas DataFrame 'feature_matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features Index(['length', 'digits', 'entropy', 'vowel-cons', 'ngrams'], dtype='object')\n",
      "   length  digits   entropy  vowel-cons       ngrams\n",
      "0      13       0  3.546594    0.300000   968.076729\n",
      "1      25      10  3.833270    0.250000   481.067222\n",
      "2      12       0  2.855389    0.090909  1036.365657\n",
      "3      26       6  3.844107    0.052632   708.328718\n",
      "4      12       0  3.084963    0.090909   897.543434\n"
     ]
    }
   ],
   "source": [
    "target = df_final['isDGA']\n",
    "feature_matrix = df_final.drop(['isDGA'], axis=1)\n",
    "print('Final features', feature_matrix.columns)\n",
    "\n",
    "print( feature_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Simple Cross-Validation\n",
    "\n",
    "Tasks:\n",
    "- split your feature matrix X and target vector into train and test subsets using sklearn [model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Cross-Validation: Split the data set into training and test data\n",
    "feature_matrix_train, feature_matrix_test, target_train, target_test = model_selection.train_test_split(feature_matrix, target, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length        1500\n",
       "digits        1500\n",
       "entropy       1500\n",
       "vowel-cons    1500\n",
       "ngrams        1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length        500\n",
       "digits        500\n",
       "entropy       500\n",
       "vowel-cons    500\n",
       "ngrams        500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179    0\n",
       "1529    0\n",
       "1125    0\n",
       "1739    0\n",
       "1303    0\n",
       "Name: isDGA, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model and make a prediction\n",
    "\n",
    "Finally, we have prepared and segmented the data. Let's start classifying!!   \n",
    "\n",
    "Tasks:\n",
    "\n",
    "-  Use the sklearn [tree.DecisionTreeClassfier()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), create a decision tree with standard parameters, and train it using the ```.fit()``` function with ```X_train``` and ```target_train``` data.\n",
    "-  Next, pull a few random rows from the data and see if your classifier got it correct.\n",
    "\n",
    "If you are interested in trying a real unknown domain, you'll have to create a function to generate the features for that domain before you run it through the classifier (see function ```is_dga``` a few cells below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n",
      "Accurate prediction? 1500    True\n",
      "Name: isDGA, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree based on the entropy criterion\n",
    "clf = tree.DecisionTreeClassifier()  # clf means classifier\n",
    "clf = clf.fit(feature_matrix_train, target_train)\n",
    "\n",
    "# Extract a row from the test data\n",
    "test_feature = feature_matrix_test[192:193]\n",
    "test_target = target_test[192:193]\n",
    "\n",
    "# Make the prediction\n",
    "pred = clf.predict(test_feature)\n",
    "print('Predicted class:', pred)\n",
    "print('Accurate prediction?', pred[0] == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>digits</th>\n",
       "      <th>entropy</th>\n",
       "      <th>vowel-cons</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>3.551109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>457.838732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>4.163856</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>656.989638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.251629</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1508.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1281.912698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.855389</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>688.598485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1606.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2.661226</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1645.425214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>4.163856</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>778.120676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3.121928</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1139.212037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.238901</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1249.631313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3.521641</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1425.548535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.238901</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>950.006410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>3.593139</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>289.297379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>766.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.725481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1366.445767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.022055</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1236.629293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1488.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.921928</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1765.397222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.419382</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1312.410053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.947703</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1254.415344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.521641</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1385.012698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.370951</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1201.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2.913977</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1388.780808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1491.962302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3.521641</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>648.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3.095795</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1286.953535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.418296</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>889.225253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.446439</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1362.980556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.725481</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1331.191138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>4.161978</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>629.523932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3.373557</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1274.368376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3.471354</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1060.748997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.921928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1829.612037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1220.394444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3.902312</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>964.977721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.664498</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1109.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>3.855389</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>297.446860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>4.066109</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1086.298535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3.121928</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1285.687963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1514.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>922.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3.277613</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1529.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.022055</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1118.463131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.947703</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1203.539021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>4.132944</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>623.070897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>3.632049</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>481.695632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>4.088221</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>520.707217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.521641</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1553.320635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>348.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.921928</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1497.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3.584963</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>957.512626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3.700440</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1265.642968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>3.736007</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>466.365622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2.913977</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1648.983165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3.272804</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1371.717168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>3.855389</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>688.413977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.446439</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1792.381481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.392747</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>759.509324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3.182006</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1606.367521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length  digits   entropy  vowel-cons       ngrams\n",
       "766       32      22  3.551109    1.000000   457.838732\n",
       "182       25       7  4.163856    0.200000   656.989638\n",
       "1763      12       0  3.251629    0.714286  1508.707071\n",
       "1814       8       0  2.750000    0.600000  1281.912698\n",
       "596       12       0  2.855389    0.200000   688.598485\n",
       "1410       6       0  2.584963    0.500000  1606.894444\n",
       "1994      13       0  2.661226    0.625000  1645.425214\n",
       "510       25       7  4.163856    0.636364   778.120676\n",
       "361       10       0  3.121928    0.428571  1139.212037\n",
       "1563      13       0  3.238901    0.625000  1249.631313\n",
       "1917      14       0  3.521641    0.555556  1425.548535\n",
       "246       13       0  3.238901    0.083333   950.006410\n",
       "371       32      22  3.593139    0.250000   289.297379\n",
       "1954       3       1  1.584963    0.000000   766.888889\n",
       "1314       9       0  2.725481    0.500000  1366.445767\n",
       "1477      12       0  3.022055    0.714286  1236.629293\n",
       "1620       8       0  3.000000    0.600000  1488.396825\n",
       "1639      10       0  2.921928    0.428571  1765.397222\n",
       "1574       9       0  2.419382    0.800000  1312.410053\n",
       "884        9       0  2.947703    0.285714  1254.415344\n",
       "1133       7       0  2.521641    0.400000  1385.012698\n",
       "1986       5       0  1.370951    1.500000  1201.222222\n",
       "1118      11       0  2.913977    0.571429  1388.780808\n",
       "1492       8       0  3.000000    0.333333  1491.962302\n",
       "317       14       0  3.521641    0.166667   648.047619\n",
       "633       11       0  3.095795    0.571429  1286.953535\n",
       "495       12       0  3.418296    0.333333   889.225253\n",
       "772       10       0  2.446439    0.666667  1362.980556\n",
       "1120       9       0  2.725481    0.333333  1331.191138\n",
       "427       26       9  4.161978    0.133333   629.523932\n",
       "...      ...     ...       ...         ...          ...\n",
       "567       15       0  3.373557    0.500000  1274.368376\n",
       "62        19       0  3.471354    0.461538  1060.748997\n",
       "1760      10       0  2.921928    1.000000  1829.612037\n",
       "1057       6       0  2.584963    0.500000  1220.394444\n",
       "665       27       0  3.902312    0.285714   964.977721\n",
       "1172       1       0 -0.000000    0.000000    61.333333\n",
       "1582       7       0  1.664498    1.333333  1109.173016\n",
       "407       24      10  3.855389    0.272727   297.446860\n",
       "22        28       0  4.066109    0.555556  1086.298535\n",
       "1329      10       0  3.121928    0.666667  1285.687963\n",
       "1017       4       0  1.500000    1.000000  1514.416667\n",
       "1402       4       0  2.000000    0.333333   922.527778\n",
       "1100      11       0  3.277613    0.571429  1529.181818\n",
       "356       12       0  3.022055    0.200000  1118.463131\n",
       "1350       9       0  2.947703    0.285714  1203.539021\n",
       "664       26       8  4.132944    0.285714   623.070897\n",
       "618       32      19  3.632049    0.625000   481.695632\n",
       "792       27      10  4.088221    0.133333   520.707217\n",
       "1340       7       0  2.521641    1.333333  1553.320635\n",
       "1580       2       0  1.000000    1.000000   348.833333\n",
       "1782      10       0  2.921928    0.666667  1497.694444\n",
       "191       12       0  3.584963    0.333333   957.512626\n",
       "1944      13       2  3.700440    0.571429  1265.642968\n",
       "692       27      14  3.736007    0.444444   466.365622\n",
       "1096      11       0  2.913977    0.833333  1648.983165\n",
       "151       21       0  3.272804    0.500000  1371.717168\n",
       "799       24       4  3.855389    0.250000   688.413977\n",
       "249       10       0  2.446439    0.666667  1792.381481\n",
       "260       13       0  3.392747    0.181818   759.509324\n",
       "1669      14       0  3.182006    0.555556  1606.367521\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For simplicity let's just copy the needed function in here again\n",
    "\n",
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    prob = [ float(x.count(c)) / len(x) for c in dict.fromkeys(list(x)) ] \n",
    "    H = - sum([ p * np.log2(p) for p in prob ]) \n",
    "    return H\n",
    "\n",
    "def vowel_consonant_ratio (x):\n",
    "    # Calculate vowel to consonant ratio\n",
    "    x = x.lower()\n",
    "    vowels_pattern = re.compile('([aeiou])')\n",
    "    consonants_pattern = re.compile('([b-df-hj-np-tv-z])')\n",
    "    vowels = re.findall(vowels_pattern, x)\n",
    "    consonants = re.findall(consonants_pattern, x)\n",
    "    try:\n",
    "        ratio = len(vowels) / len(consonants)\n",
    "    except: # catch zero devision exception \n",
    "        ratio = 0  \n",
    "    return ratio\n",
    "\n",
    "# ngrams: Implementation according to Schiavoni 2014: \"Phoenix: DGA-based Botnet Tracking and Intelligence\"\n",
    "# http://s2lab.isg.rhul.ac.uk/papers/files/dimva2014.pdf\n",
    "\n",
    "def ngrams(word, n):\n",
    "    # Extract all ngrams and return a regular Python list\n",
    "    # Input word: can be a simple string or a list of strings\n",
    "    # Input n: Can be one integer or a list of integers \n",
    "    # if you want to extract multipe ngrams and have them all in one list\n",
    "    \n",
    "    l_ngrams = []\n",
    "    if isinstance(word, list):\n",
    "        for w in word:\n",
    "            if isinstance(n, list):\n",
    "                for curr_n in n:\n",
    "                    ngrams = [w[i:i+curr_n] for i in range(0,len(w)-curr_n+1)]\n",
    "                    l_ngrams.extend(ngrams)\n",
    "            else:\n",
    "                ngrams = [w[i:i+n] for i in range(0,len(w)-n+1)]\n",
    "                l_ngrams.extend(ngrams)\n",
    "    else:\n",
    "        if isinstance(n, list):\n",
    "            for curr_n in n:\n",
    "                ngrams = [word[i:i+curr_n] for i in range(0,len(word)-curr_n+1)]\n",
    "                l_ngrams.extend(ngrams)\n",
    "        else:\n",
    "            ngrams = [word[i:i+n] for i in range(0,len(word)-n+1)]\n",
    "            l_ngrams.extend(ngrams)\n",
    "#     print(l_ngrams)\n",
    "    return l_ngrams\n",
    "\n",
    "def ngram_feature(domain, d, n):\n",
    "    # Input is your domain string or list of domain strings\n",
    "    # a dictionary object d that contains the count for most common english words\n",
    "    # finally you n either as int list or simple int defining the ngram length\n",
    "    \n",
    "    # Core magic: Looks up domain ngrams in english dictionary ngrams and sums up the \n",
    "    # respective english dictionary counts for the respective domain ngram\n",
    "    # sum is normalized\n",
    "    \n",
    "    l_ngrams = ngrams(domain, n)\n",
    "#     print(l_ngrams)\n",
    "    count_sum=0\n",
    "    for ngram in l_ngrams:\n",
    "        if d[ngram]:\n",
    "            count_sum+=d[ngram]\n",
    "    try:\n",
    "        feature = count_sum/(len(domain)-n+1)\n",
    "    except:\n",
    "        feature = 0\n",
    "    return feature\n",
    "    \n",
    "def average_ngram_feature(l_ngram_feature):\n",
    "    # input is a list of calls to ngram_feature(domain, d, n)\n",
    "    # usually you would use various n values, like 1,2,3...\n",
    "    return sum(l_ngram_feature)/len(l_ngram_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of domain spardeingeld is [0 means legit and 1 dga]:  0\n",
      "Predictions of domain google is [0 means legit and 1 dga]:  0\n",
      "Predictions of domain 1vxznov16031kjxneqjk1rtofi6 is [0 means legit and 1 dga]:  1\n",
      "Predictions of domain lthmqglxwmrwex is [0 means legit and 1 dga]:  1\n"
     ]
    }
   ],
   "source": [
    "def is_dga(domain, clf, d):\n",
    "    # Function that takes new domain string, trained model 'clf' as input and\n",
    "    # dictionary d of most common english words\n",
    "    # returns prediction\n",
    "    \n",
    "    domain_features = np.empty([1,5])\n",
    "    # order of features is ['length', 'digits', 'entropy', 'vowel-cons', 'ngrams']\n",
    "    domain_features[0,0] = len(domain)\n",
    "    pattern = re.compile('([0-9])')\n",
    "    domain_features[0,1] = len(re.findall(pattern, domain))\n",
    "    domain_features[0,2] = H_entropy(domain)\n",
    "    domain_features[0,3] = vowel_consonant_ratio(domain)\n",
    "    domain_features[0,4] = average_ngram_feature([ngram_feature(domain, d, 1), \n",
    "                                                  ngram_feature(domain, d, 2), \n",
    "                                                  ngram_feature(domain, d, 3)])\n",
    "    pred = clf.predict(domain_features)\n",
    "    return pred[0]\n",
    "\n",
    "\n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('spardeingeld'), is_dga('spardeingeld', clf, d))  \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('google'), is_dga('google', clf, d)) \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('1vxznov16031kjxneqjk1rtofi6'), is_dga('1vxznov16031kjxneqjk1rtofi6', clf, d)) \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('lthmqglxwmrwex'), is_dga('lthmqglxwmrwex', clf, d)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assess model accuracy with simple cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Make predictions for all your data. Call the ```.predict()``` method on the clf with your training data ```X_train``` and store the results in a variable called ```target_pred```.\n",
    "- Use sklearn [metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to determine your models accuracy. Detailed Instruction:\n",
    "    - Use your trained model to predict the labels of your test data ```X_test```. Run ```.predict()``` method on the clf with your test data ```X_test``` and store the results in a variable called ```target_pred```.. \n",
    "    - Then calculate the accuracy using ```target_test``` (which are the true labels/groundtruth) AND your models predictions on the test portion ```target_pred``` as inputs. The advantage here is to see how your model performs on new data it has not been seen during the training phase. The fair approach here is a simple **cross-validation**!\n",
    "    \n",
    "- Print out the confusion matrix using [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- Use Yellowbrick to visualize the classification report and confusion matrix. (http://www.scikit-yb.org/en/latest/examples/modelselect.html#common-metrics-for-evaluating-classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n",
      "Confusion Matrix\n",
      " [[219  37]\n",
      " [ 36 208]]\n"
     ]
    }
   ],
   "source": [
    "# fair approach: make prediction on test data portion\n",
    "target_pred = clf.predict(feature_matrix_test)\n",
    "print(metrics.accuracy_score(target_test, target_pred))\n",
    "print('Confusion Matrix\\n', metrics.confusion_matrix(target_test, target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      legit       0.86      0.86      0.86       256\n",
      "        dga       0.85      0.85      0.85       244\n",
      "\n",
      "avg / total       0.85      0.85      0.85       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report...neat summary\n",
    "print(metrics.classification_report(target_test, target_pred, target_names=['legit', 'dga']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85399999999999998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short-cut\n",
    "clf.score(feature_matrix_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d31c860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz = ConfusionMatrix(clf)\n",
    "viz.fit(feature_matrix_train, target_train)\n",
    "viz.score(feature_matrix_test, target_test)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAEzCAYAAAACZalSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAWziEjCKJhFRBJGwSwikjAKZhGRhPl/Xp8Pf50r5k8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d472240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz = ClassificationReport(clf)\n",
    "viz.fit(feature_matrix_train, target_train)\n",
    "viz.score(feature_matrix_test, target_test)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Assess model accuracy with k-fold cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Partition the dataset into *k* different subsets\n",
    "- Create *k* different models by training on *k-1* subsets and testing on the remaining subsets\n",
    "- Measure the performance on each of the models and take the average measure.\n",
    "\n",
    "*Short-Cut*\n",
    "All of these steps can be easily achieved by simply using sklearn's [model_selection.KFold()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) and [model_selection.cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvKFold = model_selection.KFold(n_splits=3, shuffle=True, random_state=33) \n",
    "cvKFold.get_n_splits(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86656672  0.85457271  0.86786787]\n"
     ]
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, feature_matrix, target, cv=cvKFold)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.863 (+/- 0.004)\n"
     ]
    }
   ],
   "source": [
    "# Get avergage score +- Standard Error (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.sem.html)\n",
    "from scipy.stats import sem\n",
    "def mean_score( scores ):\n",
    "    return \"Mean score: {0:.3f} (+/- {1:.3f})\".format( np.mean(scores), sem( scores ))\n",
    "print( mean_score( scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Visualizing your Tree\n",
    "As an optional step, you can actually visualize your tree.  The following code will generate a graph of your decision tree.  You will need graphviz (http://www.graphviz.org) and pydotplus (or pydot) installed for this to work.\n",
    "The Griffon VM has this installed already, but if you try this on a Mac, or Linux machine you will need to install graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used to visualize the decision tree and require that you have GraphViz\n",
    "# and pydot or pydotplus installed on your computer.\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.core.display import Image\n",
    "import pydotplus as pydot\n",
    "\n",
    "\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=['length', 'digits', 'entropy', 'vowel-cons', 'ngrams']) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the Random Forest Classifier\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=10, \n",
    "                             max_depth=None, \n",
    "                             min_samples_split=2, \n",
    "                             random_state=0)\n",
    "\n",
    "random_forest_clf = random_forest_clf.fit(feature_matrix_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create the SVM classifier\n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier = svm_classifier.fit(feature_matrix_train, target_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
